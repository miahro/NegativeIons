{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMEAR raw data loader\n",
    "\n",
    "Loads data from SMEAR API and saves to local .csv files.\n",
    "\n",
    "No other functionality (such as analysis, preprosessing, etc), the mere idea is to load the data to local files once. This is to reduce time for API queries and to reduce API load. Note that file names are managed in file ```file_config.py``` \n",
    "\n",
    "Utilizes loader functions in module ```avaa_api.py``` by pkolari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import datetime\n",
    "import utils.avaa_api as avaa_api\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from file_config import FILE_PATHS\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure path\n",
    "\n",
    "sys.path.append(\"../src\")  # Ensure src is in Python path if not already\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cell below specify parameters to be downloaded, aggregation method, and data interval. Specify these for each station separately. Also, specify timeout (for API response problems). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration\n",
    "\n",
    "stations = [\n",
    "  'var',\n",
    "  'hyy',\n",
    "  'sii',\n",
    "  'kum'\n",
    "]\n",
    "\n",
    "timeout = 60\n",
    "\n",
    "## Värriö SMEAR I station\n",
    "var_tablevars = [\n",
    "  'VAR_META.TDRY0',\n",
    "  'VAR_META.SO2_1',\n",
    "  'VAR_EDDY.u_star',\n",
    "  'VAR_META.PAR',\n",
    "  'VAR_META.RH0',\n",
    "  'VAR_META.WS00',\n",
    "  'VAR_META.WDIR',\n",
    "  'VAR_META.rainint'\n",
    "]\n",
    "\n",
    "var_interval = 30\n",
    "var_aggregation = 'ARITHMETIC'\n",
    "\n",
    "## Hyytiälä SMEAR II station \n",
    "\n",
    "\n",
    "hyy_tablevars = [\n",
    "  'HYY_EDDY233.u_star',\n",
    "  'HYY_EDDYSUB.u_star_subm',\n",
    "  'HYY_EDDYMAST.u_star_270',\n",
    "  #'HYY_EDDYTOW.u_star_radtow', insufficient data\n",
    "  #'HYY_EDDYMAST.u_star_330', # No data\n",
    "  #'HYY_EDDY233.u_star_460', # No data\n",
    "  'HYY_META.SO2168',\n",
    "  'HYY_META.RHTd',\n",
    "  'HYY_META.RH33icos',\n",
    "  'HYY_META.WS336',\n",
    "  'HYY_META.WSU336',\n",
    "  'HYY_META.WS42', # no data\n",
    "  'HYY_META.WSU42', # not cup but sonic\n",
    "  'HYY_META.WD', #avg of 16.8-67.2\n",
    "  'HYY_META.WDU336',\n",
    "  'HYY_META.T336',\n",
    "  'HYY_META.T42', # partial data\n",
    "  'HYY_META.PAR', # OK\n",
    "  'HYY_META.PAR2', # duplicate instrument for PAR\n",
    "  'HYY_META.maaPAR', #OK\n",
    "]\n",
    "\n",
    "hyy_interval = 30\n",
    "hyy_aggregation = 'ARITHMETIC'\n",
    "\n",
    "## Siikaneva SMEAR II station\n",
    "## Just example parameters in template, specify actual parameters to be downloaded\n",
    "\n",
    "sii_tablevars = [\n",
    "  'SII1_META.T_a',\n",
    "  'SII1_META.P'\n",
    "]\n",
    "\n",
    "sii_interval = 30\n",
    "sii_aggregation = 'ARITHMETIC'\n",
    "\n",
    "\n",
    "\n",
    "## Kumpula SMEAR III station\n",
    "## Just example parameters in template specify actual parameters to be downloaded\n",
    "\n",
    "kum_tablevars = [\n",
    "  'KUM_META.t',\n",
    "  'KUM_META.p'\n",
    "]\n",
    "\n",
    "kum_interval = 30\n",
    "kum_aggregation = 'ARITHMETIC'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below loads negative ion data from ```.txt``` files to dataframes and saves to local csv file. Format of source file is assumed to be:\n",
    "\n",
    "timestamp (YYYY-MM-DD HH:MM:SS) concentration (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kumpula negative ions data from 2016-04-20 01:30:00 to 2024-11-11 23:30:00\n",
      "Värriö negative ions data from 2019-02-09 00:00:00 to 2024-11-11 23:30:00\n",
      "Hyytiälä negative ions data from 2010-02-02 00:00:00 to 2024-08-31 23:30:00\n",
      "Siikaneva negative ions data from 2019-10-28 00:00:00 to 2024-12-17 23:30:00\n"
     ]
    }
   ],
   "source": [
    "## Load negative ion data\n",
    "\n",
    "\n",
    "kum_neg_ions_raw_df = pd.read_csv(FILE_PATHS['kum_neg_ions_txt'], sep=',', header=0, parse_dates=[0])\n",
    "kum_start_date = kum_neg_ions_raw_df.iloc[0]['date']\n",
    "kum_end_date = kum_neg_ions_raw_df.iloc[-1]['date']\n",
    "kum_neg_ions_raw_df.to_csv(FILE_PATHS['kum_neg_ions_csv'], index=False)\n",
    "\n",
    "var_neg_ions_raw_df = pd.read_csv(FILE_PATHS['var_neg_ions_txt'], sep=',', header=0, parse_dates=[0])\n",
    "var_start_date = var_neg_ions_raw_df.iloc[0]['date'].to_pydatetime()\n",
    "var_end_date = var_neg_ions_raw_df.iloc[-1]['date'].to_pydatetime()\n",
    "var_neg_ions_raw_df.to_csv(FILE_PATHS['var_neg_ions_csv'], index=False)\n",
    "\n",
    "hyy_neg_ions_raw_df = pd.read_csv(FILE_PATHS['hyy_neg_ions_txt'], sep=',', header=0, parse_dates=[0])\n",
    "hyy_start_date = hyy_neg_ions_raw_df.iloc[0]['date'].to_pydatetime()\n",
    "hyy_end_date = hyy_neg_ions_raw_df.iloc[-1]['date'].to_pydatetime()\n",
    "hyy_neg_ions_raw_df.to_csv(FILE_PATHS['hyy_neg_ions_csv'], index=False)\n",
    "\n",
    "sii_neg_ions_raw_df = pd.read_csv(FILE_PATHS['sii_neg_ions_txt'], sep=',', header=0, parse_dates=[0])\n",
    "sii_start_date = sii_neg_ions_raw_df.iloc[0]['date'].to_pydatetime()\n",
    "sii_end_date = sii_neg_ions_raw_df.iloc[-1]['date'].to_pydatetime()\n",
    "sii_neg_ions_raw_df.to_csv(FILE_PATHS['sii_neg_ions_csv'], index=False)\n",
    "\n",
    "print('Kumpula negative ions data from', kum_start_date, 'to', kum_end_date)\n",
    "print('Värriö negative ions data from', var_start_date, 'to', var_end_date)\n",
    "print('Hyytiälä negative ions data from', hyy_start_date, 'to', hyy_end_date)\n",
    "print('Siikaneva negative ions data from', sii_start_date, 'to', sii_end_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below loads metadata for all stations and all parameters specified to be downloaded and saves to local ```.csv```files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for all paramter for Hyytiälä loaded successfully: True\n",
      "Metadata for all paramter for Värriö loaded successfully: True\n",
      "Metadata for all paramter for Siikaneva loaded successfully: True\n",
      "Metadata for all paramter for Kumpula loaded successfully: True\n"
     ]
    }
   ],
   "source": [
    "# Load and record metadata\n",
    "\n",
    "hyy_meta_data = avaa_api.getMetadata(tablevariables=hyy_tablevars, fmt='csv')\n",
    "print(f\"Metadata for all paramter for Hyytiälä loaded successfully: {len(hyy_meta_data)==len(hyy_tablevars)}\")\n",
    "hyy_meta_data.to_csv(FILE_PATHS['hyy_metadata'], index=False)\n",
    "\n",
    "var_meta_data = avaa_api.getMetadata(tablevariables=var_tablevars, fmt='csv')\n",
    "print(f\"Metadata for all paramter for Värriö loaded successfully: {len(var_meta_data)==len(var_tablevars)}\")\n",
    "var_meta_data.to_csv(FILE_PATHS['var_metadata'], index=False)\n",
    "\n",
    "\n",
    "sii_meta_data = avaa_api.getMetadata(tablevariables=sii_tablevars, fmt='csv')\n",
    "print(f\"Metadata for all paramter for Siikaneva loaded successfully: {len(sii_meta_data)==len(sii_tablevars)}\")\n",
    "sii_meta_data.to_csv(FILE_PATHS['sii_metadata'], index=False)\n",
    "\n",
    "kum_meta_data = avaa_api.getMetadata(tablevariables=kum_tablevars, fmt='csv')\n",
    "print(f\"Metadata for all paramter for Kumpula loaded successfully: {len(kum_meta_data)==len(kum_tablevars)}\")\n",
    "kum_meta_data.to_csv(FILE_PATHS['kum_metadata'], index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Värriö data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var start date: 2019-02-09 00:00:00\n",
      "var end date: 2024-11-11 23:30:00\n",
      "start: 2019-02-09 00:00:00, end: 2020-02-09 00:00:00\n",
      "start: 2020-02-09 00:00:00, end: 2021-02-09 00:00:00\n",
      "start: 2021-02-09 00:00:00, end: 2022-02-09 00:00:00\n",
      "start: 2022-02-09 00:00:00, end: 2023-02-09 00:00:00\n",
      "start: 2023-02-09 00:00:00, end: 2024-02-09 00:00:00\n",
      "start: 2024-02-09 00:00:00, end: 2024-11-11 23:30:00\n",
      "                  Datetime  VAR_META.TDRY0  VAR_META.SO2_1  VAR_EDDY.u_star  \\\n",
      "count               100943   100564.000000     92171.00000     97088.000000   \n",
      "mean   2021-12-26 11:30:00        0.748549         0.28997         0.595818   \n",
      "min    2019-02-09 00:00:00      -39.130200        -0.24400         0.009200   \n",
      "25%    2020-07-18 17:45:00       -6.743037        -0.00467         0.383590   \n",
      "50%    2021-12-26 11:30:00       -0.352400         0.02333         0.580900   \n",
      "75%    2023-06-05 05:15:00        8.529760         0.11600         0.787312   \n",
      "max    2024-11-11 23:00:00       30.561840        39.91333         1.856550   \n",
      "std                    NaN        9.919913         1.17158         0.279843   \n",
      "\n",
      "        VAR_META.PAR   VAR_META.RH0  VAR_META.WS00  VAR_META.WDIR  \\\n",
      "count  100572.000000  100561.000000  100515.000000  100579.000000   \n",
      "mean      176.572656      81.859686       3.537042     191.227693   \n",
      "min        -2.280000      16.090000       0.164540       0.000000   \n",
      "25%         0.000000      71.744000       2.429685     115.000250   \n",
      "50%        13.794000      88.114000       3.360000     215.416670   \n",
      "75%       230.394000      96.076670       4.476030     254.905000   \n",
      "max      1633.240000      99.743330      11.605500     358.203330   \n",
      "std       297.409175      17.932095       1.499478      78.440902   \n",
      "\n",
      "       VAR_META.rainint  \n",
      "count      99447.000000  \n",
      "mean           0.042979  \n",
      "min            0.000000  \n",
      "25%            0.000000  \n",
      "50%            0.000000  \n",
      "75%            0.000000  \n",
      "max           32.507200  \n",
      "std            0.348402  \n"
     ]
    }
   ],
   "source": [
    "#Load Värriö data and save dataframe to CSV\n",
    "#load 1 year at a time due to API limitations\n",
    "\n",
    "print(f\"var start date: {var_start_date}\")\n",
    "print(f\"var end date: {var_end_date}\")\n",
    "\n",
    "var_years = var_end_date.year - var_start_date.year\n",
    "\n",
    "partial_datafames = []\n",
    "start = var_start_date\n",
    "\n",
    "for i in range(0, var_years+1):\n",
    "  end = start + relativedelta(years=1)\n",
    "  if end > var_end_date:\n",
    "    end = var_end_date\n",
    "  print(f\"start: {start}, end: {end}\")\n",
    "  partial_datafames.append(avaa_api.getData(fdate=start, ldate=end, tablevariables=var_tablevars, interval=var_interval, aggregation=var_aggregation, timeout=timeout))\n",
    "  start = end\n",
    "\n",
    "var_data = pd.concat(partial_datafames)\n",
    "\n",
    "\n",
    "print(var_data.describe())\n",
    "var_data.to_csv(FILE_PATHS['var_raw'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Hyytiälä data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyy start date: 2010-02-02 00:00:00\n",
      "hyy end date: 2024-08-31 23:30:00\n",
      "years: 14\n",
      "round 0, fetching data for 2010-02-02 00:00:00 - 2011-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYMAST.u_star_270\n",
      "round 1, fetching data for 2011-02-02 00:00:00 - 2012-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYMAST.u_star_270\n",
      "round 2, fetching data for 2012-02-02 00:00:00 - 2013-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYMAST.u_star_270\n",
      "round 3, fetching data for 2013-02-02 00:00:00 - 2014-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYMAST.u_star_270\n",
      "round 4, fetching data for 2014-02-02 00:00:00 - 2015-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYMAST.u_star_270\n",
      "round 5, fetching data for 2015-02-02 00:00:00 - 2016-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYMAST.u_star_270\n",
      "round 6, fetching data for 2016-02-02 00:00:00 - 2017-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYMAST.u_star_270\n",
      "round 7, fetching data for 2017-02-02 00:00:00 - 2018-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYSUB.u_star_subm\n",
      "round 8, fetching data for 2018-02-02 00:00:00 - 2019-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYSUB.u_star_subm\n",
      "round 9, fetching data for 2019-02-02 00:00:00 - 2020-02-02 00:00:00\n",
      "round 10, fetching data for 2020-02-02 00:00:00 - 2021-02-02 00:00:00\n",
      "round 11, fetching data for 2021-02-02 00:00:00 - 2022-02-02 00:00:00\n",
      "round 12, fetching data for 2022-02-02 00:00:00 - 2023-02-02 00:00:00\n",
      "round 13, fetching data for 2023-02-02 00:00:00 - 2024-02-02 00:00:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYSUB.u_star_subm\n",
      "round 14, fetching data for 2024-02-02 00:00:00 - 2024-08-31 23:30:00\n",
      "WARNING! Temporal coverage of some table(s) is outside the given time span.\n",
      "These columns will be missing:\n",
      "   HYY_EDDYSUB.u_star_subm\n",
      "                            Datetime  HYY_EDDY233.u_star  \\\n",
      "count                         255599       148514.000000   \n",
      "mean   2017-05-18 11:30:00.000001024            0.501713   \n",
      "min              2010-02-02 00:00:00            0.006310   \n",
      "25%              2013-09-25 05:45:00            0.296822   \n",
      "50%              2017-05-18 11:30:00            0.465055   \n",
      "75%              2021-01-08 17:15:00            0.660160   \n",
      "max              2024-08-31 23:00:00           13.137160   \n",
      "std                              NaN            0.353294   \n",
      "\n",
      "       HYY_EDDYSUB.u_star_subm  HYY_META.SO2168  HYY_META.RHTd  \\\n",
      "count            108829.000000    231235.000000  206610.000000   \n",
      "mean                  0.103376         0.137527      79.760642   \n",
      "min                   0.000000        -0.112000      13.215330   \n",
      "25%                   0.060260         0.005330      66.367752   \n",
      "50%                   0.093220         0.033000      88.075300   \n",
      "75%                   0.137250         0.120670      96.104670   \n",
      "max                   0.640040        22.625000     103.830000   \n",
      "std                   0.057198         0.379152      20.078652   \n",
      "\n",
      "       HYY_META.RH33icos  HYY_META.WS336  HYY_META.WSU336  HYY_META.WS42  \\\n",
      "count       93599.000000    81200.000000    249625.000000            0.0   \n",
      "mean           82.886023        3.166050         3.292904            NaN   \n",
      "min            18.380330        0.030000         0.114670            NaN   \n",
      "25%            72.775665        2.109325         2.287670            NaN   \n",
      "50%            91.304640        3.067330         3.149330            NaN   \n",
      "75%            97.452750        4.091682         4.113210            NaN   \n",
      "max           100.000000       11.398670        11.370000            NaN   \n",
      "std            19.137122        1.567632         1.423673            NaN   \n",
      "\n",
      "       HYY_META.WSU42    HYY_META.WD  HYY_META.WDU336  HYY_META.T336  \\\n",
      "count    38091.000000  174481.000000    249645.000000  243324.000000   \n",
      "mean         0.735521     192.016554       195.044924       4.665399   \n",
      "min          0.059580       0.920000         2.000000     -27.563000   \n",
      "25%          0.472355     131.800000       132.366670      -1.602330   \n",
      "50%          0.695700     198.633330       207.820500       3.980290   \n",
      "75%          0.948140     256.120000       258.470590      12.126000   \n",
      "max          2.768620     359.986670       358.000000      31.576000   \n",
      "std          0.351860      81.421177        83.392638       9.533476   \n",
      "\n",
      "        HYY_META.T42   HYY_META.PAR  HYY_META.PAR2  HYY_META.maaPAR  \\\n",
      "count  186912.000000  253449.000000  236868.000000    234782.000000   \n",
      "mean        5.521629     207.915940     207.901376        49.045114   \n",
      "min       -27.036330     -38.290000     -36.948330        -7.870000   \n",
      "25%        -0.881000      -0.077790       0.000000         0.007360   \n",
      "50%         5.485670      11.480060       7.890105         1.878285   \n",
      "75%        12.851330     264.610000     261.444833        46.661690   \n",
      "max        32.566670    1922.350830    1917.269330      1407.404330   \n",
      "std         9.550627     353.799483     356.694744       106.169934   \n",
      "\n",
      "       HYY_EDDYMAST.u_star_270  \n",
      "count            100176.000000  \n",
      "mean                  0.511543  \n",
      "min                   0.007540  \n",
      "25%                   0.304490  \n",
      "50%                   0.480305  \n",
      "75%                   0.685800  \n",
      "max                   1.859490  \n",
      "std                   0.272683  \n"
     ]
    }
   ],
   "source": [
    "# Load Hyytiälä data and save dataframe to CSV\n",
    "# load 1 year at a time due to API limitations\n",
    "\n",
    "\n",
    "print(f\"hyy start date: {hyy_start_date}\")\n",
    "print(f\"hyy end date: {hyy_end_date}\")\n",
    "\n",
    "hyy_years = hyy_end_date.year - hyy_start_date.year\n",
    "print(f\"years: {hyy_years}\")\n",
    "\n",
    "\n",
    "partial_dataframes = []\n",
    "start = hyy_start_date\n",
    "\n",
    "for i in range(0, hyy_years+1):\n",
    "  end = start + relativedelta(years=1)\n",
    "  if end > hyy_end_date:\n",
    "    end = hyy_end_date\n",
    "  print(f\"round {i}, fetching data for {start} - {end}\")\n",
    "  partial_data = avaa_api.getData(fdate=start, ldate=end, tablevariables=hyy_tablevars, interval=hyy_interval, aggregation=hyy_aggregation, timeout=timeout)\n",
    "  partial_dataframes.append(partial_data)\n",
    "  start = end\n",
    "\n",
    "hyy_data = pd.concat(partial_dataframes)\n",
    "hyy_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(hyy_data.describe())\n",
    "hyy_data.to_csv(FILE_PATHS['hyy_raw'], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Siikaneva data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sii start date: 2019-10-28 00:00:00\n",
      "sii end date: 2024-12-17 23:30:00\n",
      "years: 5\n",
      "round 0, fetching data for 2019-10-28 00:00:00 - 2020-10-28 00:00:00\n",
      "round 1, fetching data for 2020-10-28 00:00:00 - 2021-10-28 00:00:00\n",
      "round 2, fetching data for 2021-10-28 00:00:00 - 2022-10-28 00:00:00\n",
      "round 3, fetching data for 2022-10-28 00:00:00 - 2023-10-28 00:00:00\n",
      "round 4, fetching data for 2023-10-28 00:00:00 - 2024-10-28 00:00:00\n",
      "round 5, fetching data for 2024-10-28 00:00:00 - 2024-12-17 23:30:00\n",
      "                  Datetime  SII1_META.T_a   SII1_META.P\n",
      "count                90143   89768.000000  88063.000000\n",
      "mean   2022-05-23 23:30:00       4.718504    991.064356\n",
      "min    2019-10-28 00:00:00     -32.328500    937.638620\n",
      "25%    2021-02-08 11:45:00      -1.660735    983.810305\n",
      "50%    2022-05-23 23:30:00       3.668585    992.309120\n",
      "75%    2023-09-05 11:15:00      12.159568    999.360275\n",
      "max    2024-12-17 23:00:00      31.713000   1024.253800\n",
      "std                    NaN       9.806667     11.947440\n"
     ]
    }
   ],
   "source": [
    "# Load Siikaneva data and save dataframe to CSV\n",
    "# load 1 year at a time due to API limitations\n",
    "\n",
    "print(f\"sii start date: {sii_start_date}\")\n",
    "print(f\"sii end date: {sii_end_date}\")\n",
    "\n",
    "sii_years = sii_end_date.year - sii_start_date.year\n",
    "print(f\"years: {sii_years}\")\n",
    "\n",
    "\n",
    "partial_dataframes = []\n",
    "start = sii_start_date\n",
    "\n",
    "for i in range(0, sii_years+1):\n",
    "  end = start + relativedelta(years=1)\n",
    "  if end > sii_end_date:\n",
    "    end = sii_end_date\n",
    "  print(f\"round {i}, fetching data for {start} - {end}\")\n",
    "  partial_data = avaa_api.getData(fdate=start, ldate=end, tablevariables=sii_tablevars, interval=sii_interval, aggregation=sii_aggregation, timeout=timeout)\n",
    "  partial_dataframes.append(partial_data)\n",
    "  start = end\n",
    "\n",
    "sii_data = pd.concat(partial_dataframes)\n",
    "sii_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(sii_data.describe())\n",
    "sii_data.to_csv(FILE_PATHS['sii_raw'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kumpula data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kum start date: 2016-04-20 01:30:00\n",
      "kum end date: 2024-11-11 23:30:00\n",
      "years: 8\n",
      "round 0, fetching data for 2016-04-20 01:30:00 - 2017-04-20 01:30:00\n",
      "round 1, fetching data for 2017-04-20 01:30:00 - 2018-04-20 01:30:00\n",
      "round 2, fetching data for 2018-04-20 01:30:00 - 2019-04-20 01:30:00\n",
      "round 3, fetching data for 2019-04-20 01:30:00 - 2020-04-20 01:30:00\n",
      "round 4, fetching data for 2020-04-20 01:30:00 - 2021-04-20 01:30:00\n",
      "round 5, fetching data for 2021-04-20 01:30:00 - 2022-04-20 01:30:00\n",
      "round 6, fetching data for 2022-04-20 01:30:00 - 2023-04-20 01:30:00\n",
      "round 7, fetching data for 2023-04-20 01:30:00 - 2024-04-20 01:30:00\n",
      "round 8, fetching data for 2024-04-20 01:30:00 - 2024-11-11 23:30:00\n",
      "                            Datetime     KUM_META.t     KUM_META.p\n",
      "count                         150140  149966.000000  149966.000000\n",
      "mean   2020-08-01 00:14:59.999999488       7.486667    1006.167295\n",
      "min              2016-04-20 01:30:00     -21.593330     955.530000\n",
      "25%              2018-06-11 00:52:30       0.953330     999.260000\n",
      "50%              2020-08-01 00:15:00       7.053330    1006.850000\n",
      "75%              2022-09-21 23:37:30      14.816670    1013.740000\n",
      "max              2024-11-11 23:00:00      31.630000    1041.960000\n",
      "std                              NaN       8.732907      11.394983\n"
     ]
    }
   ],
   "source": [
    "# Load Kumpula data and save dataframe to CSV\n",
    "# load 1 year at a time due to API limitations\n",
    "\n",
    "\n",
    "\n",
    "print(f\"kum start date: {kum_start_date}\")\n",
    "print(f\"kum end date: {kum_end_date}\")\n",
    "\n",
    "kum_years = kum_end_date.year - kum_start_date.year\n",
    "print(f\"years: {kum_years}\")\n",
    "\n",
    "\n",
    "partial_dataframes = []\n",
    "start = kum_start_date\n",
    "\n",
    "for i in range(0, kum_years+1):\n",
    "  end = start + relativedelta(years=1)\n",
    "  if end > kum_end_date:\n",
    "    end = kum_end_date\n",
    "  print(f\"round {i}, fetching data for {start} - {end}\")\n",
    "  partial_data = avaa_api.getData(fdate=start, ldate=end, tablevariables=kum_tablevars, interval=kum_interval, aggregation=kum_aggregation, timeout=timeout)\n",
    "  partial_dataframes.append(partial_data)\n",
    "  start = end\n",
    "\n",
    "kum_data = pd.concat(partial_dataframes)\n",
    "kum_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(kum_data.describe())\n",
    "kum_data.to_csv(FILE_PATHS['kum_raw'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
